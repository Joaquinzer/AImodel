{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: alfa romeo - Number of Files: 444\n",
      "Folder: Audi - Number of Files: 324\n",
      "Folder: Bentley - Number of Files: 410\n",
      "Folder: Benz - Number of Files: 255\n",
      "Folder: Bmw - Number of Files: 277\n",
      "Folder: Cadillac - Number of Files: 384\n",
      "Folder: Dodge - Number of Files: 373\n",
      "Folder: Ferrari - Number of Files: 358\n",
      "Folder: Ford - Number of Files: 373\n",
      "Folder: Ford mustang - Number of Files: 433\n",
      "Folder: hyundai - Number of Files: 318\n",
      "Folder: Kia - Number of Files: 365\n",
      "Folder: Lamborghini - Number of Files: 419\n",
      "Folder: Lexus - Number of Files: 397\n",
      "Folder: Maserati - Number of Files: 421\n",
      "Folder: Porsche - Number of Files: 307\n",
      "Folder: Rolls royce - Number of Files: 384\n",
      "Folder: Tesla - Number of Files: 261\n",
      "Folder: Toyota - Number of Files: 317\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the folder containing your data\n",
    "folder_path = r'D:\\Documentos\\Semestre8\\Aplicaciones avanzadas\\AI\\Proyecto\\AImodel\\images\\train'\n",
    "\n",
    "# Iterate over each subfolder\n",
    "for subfolder in os.listdir(folder_path):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Count the number of files in the subfolder\n",
    "        file_count = len(os.listdir(subfolder_path))\n",
    "\n",
    "        print(f\"Folder: {subfolder} - Number of Files: {file_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2.]\n",
      " [1. 1.]]\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "preddic = [[1,1], [1,0], [0,1], [0,0], [0,1], [1,1], [1,1]]\n",
    "sum = 0\n",
    "matriz = np.zeros((2, 2))\n",
    "for i in preddic:\n",
    "   if i[0] == 1 and i[1] == 1:\n",
    "      matriz[0][0] += 1\n",
    "   elif i[0] == 0 and i[1] == 0:\n",
    "      matriz[1][1] += 1 \n",
    "   elif i[0] == 1 and i[1] == 0:\n",
    "      matriz[1][0] += 1 \n",
    "   elif i[0] == 0 and i[1] == 1:\n",
    "      matriz[0][1] += 1 \n",
    "\n",
    "print(matriz)\n",
    "print(matriz[0][0]/(matriz[0][0]+matriz[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6820 images belonging to 19 classes.\n",
      "Found 2282 images belonging to 19 classes.\n",
      "Found 2278 images belonging to 19 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = r'D:\\Documentos\\Semestre8\\Aplicaciones avanzadas\\AI\\Proyecto\\AImodel\\images\\train'\n",
    "test_data_dir = r'D:\\Documentos\\Semestre8\\Aplicaciones avanzadas\\AI\\Proyecto\\AImodel\\images\\test'\n",
    "validation_data_dir = r'D:\\Documentos\\Semestre8\\Aplicaciones avanzadas\\AI\\Proyecto\\AImodel\\images\\validation'\n",
    "\n",
    "total_train_samples = sum([len(files) for _, _, files in os.walk(train_data_dir)])\n",
    "\n",
    "train_datagen = ImageDataGenerator(1./255)  \n",
    "\n",
    "test_datagen = ImageDataGenerator(1./255)   \n",
    "\n",
    "validation_datagen = ImageDataGenerator(1./255)  \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=40,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=2000,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 148, 148, 10)      280       \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 219040)            0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               56074496  \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 19)                4883      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,079,659\n",
      "Trainable params: 56,079,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(10, (3, 3), activation=\"relu\", input_shape = (150,150,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.Dense(19,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "\t\t\t\t\t\toptimizer='Adam',\n",
    "\t\t\t\t\t\tmetrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joako\\AppData\\Local\\Temp\\ipykernel_10644\\1183620081.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 122s 713ms/step - loss: 1297.5839 - acc: 0.0930 - val_loss: 14.0885 - val_acc: 0.1500\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 120s 702ms/step - loss: 3.0110 - acc: 0.7031 - val_loss: 13.4497 - val_acc: 0.1375\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 116s 680ms/step - loss: 0.7464 - acc: 0.9235 - val_loss: 14.1762 - val_acc: 0.1813\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 114s 670ms/step - loss: 0.5353 - acc: 0.9654 - val_loss: 15.3066 - val_acc: 0.2062\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 114s 667ms/step - loss: 0.5546 - acc: 0.9611 - val_loss: 16.1352 - val_acc: 0.2000\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 114s 669ms/step - loss: 0.7028 - acc: 0.9614 - val_loss: 22.5905 - val_acc: 0.1750\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 114s 671ms/step - loss: 0.4978 - acc: 0.9696 - val_loss: 17.2771 - val_acc: 0.1875\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 114s 667ms/step - loss: 0.5305 - acc: 0.9717 - val_loss: 18.2010 - val_acc: 0.2562\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 114s 671ms/step - loss: 0.3974 - acc: 0.9760 - val_loss: 21.8527 - val_acc: 0.1875\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 113s 665ms/step - loss: 0.5805 - acc: 0.9642 - val_loss: 21.7798 - val_acc: 0.1937\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, \n",
    "\t\t\t\tsteps_per_epoch =total_train_samples/40,  \n",
    "\t\t\t\tepochs =10,\n",
    "                validation_data=validation_generator,\n",
    "            \tvalidation_steps=8 \n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step - loss: 21.1681 - acc: 0.1820\n",
      "\n",
      "test acc :\n",
      " 0.18199999630451202\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator, steps = 1)\n",
    "print('\\ntest acc :\\n', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 38ms/step\n",
      "tf.Tensor(\n",
      "[[18  4  4  7  9  5  2  2  4  5  3  1  6  2  3  4  8  5  3]\n",
      " [ 3 15  3  6  3  8  2 10 10  4  4  4  4  9  5  2  5  5  5]\n",
      " [ 3  3  7  0  1  3  1  3  2  2  1  1  2  1  5  1  3  1  1]\n",
      " [ 2  4  0  9  5  1  2  3  2  5  5  1  4  5  2  1  3  5  1]\n",
      " [ 7  8  7  2 17  8  2  4  5  7 11  7 14  0  7  4  7  4  6]\n",
      " [ 0  1  1  1  3 18  5  2  1  3  1  2  1  2  2  1  0  1  1]\n",
      " [ 6  9  3  4 10 17 49 13 15  8 24  7  9  9  6 17 11 23 11]\n",
      " [ 4  3  1  2  7  6  1 19  2  8  6  5  5  3  6  5  3  4  8]\n",
      " [ 1  3  3  3  1  2  2  6 15  2  4  1  1  3  1  0  1  1  3]\n",
      " [ 8 17 14 10 13  9  7 11 11 23  7 20  2 10  9  2 15  7  8]\n",
      " [ 2  1  0  2  2  0  1  2  4  1 16  5  0  3  2  2  1  1  1]\n",
      " [10  3  1  6  8  4  5  4  7  6  7 18 11  2 14  2  5  3  7]\n",
      " [ 4  3  2  4  3  3  5  4  4  3  3  6 16  4  3  1  3  3  4]\n",
      " [ 8 15  8  6  4  5  6  5  7  7 11  9 12 27 11  7  5  8  7]\n",
      " [ 3  5  3  1  3  2  0  3  7  0  3  7 11  3 19  1  3  4  1]\n",
      " [ 3  6  2  0  3  4  6  5  4  3  1  3  7  2  5 20  3 12  3]\n",
      " [ 2  0  2  3  2  2  2  0  1  0  3  3  1  2  3  0  6  4  1]\n",
      " [12 21  9 14 14 10 13  9 22 13 14 14 14  8 12  4 10 33 12]\n",
      " [ 3  0  4  2  3  1  0  2  1  2  0  0  3  1  0  2  2  4  8]], shape=(19, 19), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_imgs = test_generator[0][0]\n",
    "test_labels = test_generator[0][1]\n",
    "\n",
    "\n",
    "predictions = model.predict(test_imgs)\n",
    "classes_x = np.argmax(predictions,axis=1)\n",
    "classex_y = np.argmax(test_labels,axis=1)\n",
    "\n",
    "from tensorflow.math import confusion_matrix\n",
    "mat = confusion_matrix(classes_x, classex_y)\n",
    "print(mat) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
